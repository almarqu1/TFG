{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "756255b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API de Gemini configurada correctamente.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm # Usar la versión de tqdm para notebooks\n",
    "\n",
    "# --- CONFIGURACIÓN DE LA API KEY ---\n",
    "\n",
    "try:\n",
    "    dotenv.load_dotenv()  # Cargar variables de entorno desde el archivo .env\n",
    "    API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not API_KEY:\n",
    "        if 'API_KEY' not in locals():\n",
    "             raise ValueError(\"API Key no encontrada. Configúrala como variable de entorno o directamente en la celda.\")\n",
    "    \n",
    "    genai.configure(api_key=API_KEY)\n",
    "    print(\"✅ API de Gemini configurada correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error de configuración: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb431a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento seleccionado: 'P-01_zero_shot'\n",
      "Usando plantilla de prompt: 'c:\\Users\\zveng\\Desktop\\TFG\\report\\prompts\\P-01_zero_shot.txt'\n"
     ]
    }
   ],
   "source": [
    "project_root = Path.cwd().parent # Asume que el notebook está en la carpeta 'notebooks/'\n",
    "\n",
    "def load_yaml_config(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def load_and_format_prompt(template_path, cv_text, offer_text):\n",
    "    with open(template_path, 'r', encoding='utf-8') as f:\n",
    "        prompt_template = f.read()\n",
    "    return prompt_template.format(text_of_job_posting=offer_text, text_of_cv=cv_text)\n",
    "\n",
    "# --- SELECCIONA EL EXPERIMENTO A PROBAR ---\n",
    "PROMPTS_CONFIG_FILE = project_root / 'config' / 'experiment_prompts.yaml'\n",
    "prompt_name_to_test = \"P-01_zero_shot\" \n",
    "\n",
    "prompts_config = load_yaml_config(PROMPTS_CONFIG_FILE)\n",
    "chosen_prompt_config = prompts_config[prompt_name_to_test]\n",
    "prompt_template_path = project_root / chosen_prompt_config['path']\n",
    "\n",
    "print(f\"Experimento seleccionado: '{prompt_name_to_test}'\")\n",
    "print(f\"Usando plantilla de prompt: '{prompt_template_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae1260d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "✅ Datos cargados. Se procesarán 360 pares en total (en la ejecución completa).\n",
      "Ejemplo de los primeros pares a procesar:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>candidate_id</th>\n",
       "      <th>category</th>\n",
       "      <th>score</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3902322961</td>\n",
       "      <td>cand_3645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3888939035</td>\n",
       "      <td>cand_4295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3901963567</td>\n",
       "      <td>cand_2931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3904057270</td>\n",
       "      <td>cand_3252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3899522876</td>\n",
       "      <td>cand_2325</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       job_id candidate_id  category  score  annotator_id  justification\n",
       "0  3902322961    cand_3645       NaN    NaN           NaN            NaN\n",
       "1  3888939035    cand_4295       NaN    NaN           NaN            NaN\n",
       "2  3901963567    cand_2931       NaN    NaN           NaN            NaN\n",
       "3  3904057270    cand_3252       NaN    NaN           NaN            NaN\n",
       "4  3899522876    cand_2325       NaN    NaN           NaN            NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rutas a los datos\n",
    "PROCESSED_DATA_DIR = project_root / 'data' / '01_processed'\n",
    "TEST_SETS_DIR = project_root / 'data' / '02_test_sets'\n",
    "\n",
    "print(\"Cargando datos...\")\n",
    "# Cargar el archivo que define QUÉ pares vamos a procesar\n",
    "pairs_df = pd.read_csv(TEST_SETS_DIR / 'test_set_pairs_to_annotate.csv', dtype={'job_id': str, 'candidate_id': str})\n",
    "\n",
    "# Cargar los datos de ofertas y CVs para obtener los textos\n",
    "offers_df = pd.read_csv(PROCESSED_DATA_DIR / 'offers_processed.csv', index_col='job_id', dtype={'job_id': str})\n",
    "cvs_df = pd.read_csv(PROCESSED_DATA_DIR / 'cvs_processed.csv', index_col='candidate_id', dtype={'candidate_id': str})\n",
    "\n",
    "# Mapear los textos a diccionarios para un acceso rápido\n",
    "offer_texts = offers_df['description'].to_dict()\n",
    "cv_texts = cvs_df['responsibilities'].to_dict()\n",
    "\n",
    "print(f\"✅ Datos cargados. Se procesarán {len(pairs_df)} pares en total (en la ejecución completa).\")\n",
    "print(\"Ejemplo de los primeros pares a procesar:\")\n",
    "display(pairs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9105cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando prueba con 2 pares...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaa5daca1ef4fdbab342930c5074a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Probando Pares:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ Error en par (Job: 3902322961, Candidate: cand_3645): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 100\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "]\n",
      "\n",
      "❌ Error en par (Job: 3888939035, Candidate: cand_4295): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 100\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 22\n",
      "}\n",
      "]\n",
      "\n",
      "\n",
      "--- Resultados de la Prueba ---\n",
      "{\n",
      "  \"job_id\": \"3902322961\",\n",
      "  \"candidate_id\": \"cand_3645\",\n",
      "  \"error\": \"429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerDayPerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.5-pro\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n  quota_value: 100\\n}\\n, links {\\n  description: \\\"Learn more about Gemini API quotas\\\"\\n  url: \\\"https://ai.google.dev/gemini-api/docs/rate-limits\\\"\\n}\\n, retry_delay {\\n  seconds: 23\\n}\\n]\"\n",
      "}\n",
      "{\n",
      "  \"job_id\": \"3888939035\",\n",
      "  \"candidate_id\": \"cand_4295\",\n",
      "  \"error\": \"429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerDayPerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.5-pro\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n  quota_value: 100\\n}\\n, links {\\n  description: \\\"Learn more about Gemini API quotas\\\"\\n  url: \\\"https://ai.google.dev/gemini-api/docs/rate-limits\\\"\\n}\\n, retry_delay {\\n  seconds: 22\\n}\\n]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURACIÓN PARA LA PRUEBA ---\n",
    "NUM_SAMPLES_TO_TEST = 2\n",
    "small_batch_df = pairs_df.head(NUM_SAMPLES_TO_TEST)\n",
    "\n",
    "print(f\"Iniciando prueba con {NUM_SAMPLES_TO_TEST} pares...\")\n",
    "\n",
    "# Configuración del modelo Gemini\n",
    "MODEL_NAME = \"gemini-2.5-pro\"\n",
    "generation_config = genai.GenerationConfig(temperature=0.1)\n",
    "model = genai.GenerativeModel(MODEL_NAME, generation_config=generation_config)\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in tqdm(small_batch_df.iterrows(), total=len(small_batch_df), desc=\"Probando Pares\"):\n",
    "    job_id, candidate_id = row['job_id'], row['candidate_id']\n",
    "    offer_text, cv_text = offer_texts.get(job_id), cv_texts.get(candidate_id)\n",
    "\n",
    "    if not all([offer_text, cv_text]):\n",
    "        print(f\"Saltando par por datos faltantes (Job: {job_id}, Candidate: {candidate_id})\")\n",
    "        continue\n",
    "\n",
    "    prompt = load_and_format_prompt(prompt_template_path, cv_text, offer_text)\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        json_str = response.text.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "        result_json = json.loads(json_str)\n",
    "        result_json.update({'job_id': job_id, 'candidate_id': candidate_id})\n",
    "        results.append(result_json)\n",
    "        print(f\"\\n✅ Éxito en par (Job: {job_id})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error en par (Job: {job_id}, Candidate: {candidate_id}): {e}\")\n",
    "        error_data = {'job_id': job_id, 'candidate_id': candidate_id, 'error': str(e)}\n",
    "        results.append(error_data)\n",
    "\n",
    "    time.sleep(1.1)\n",
    "\n",
    "print(\"\\n\\n--- Resultados de la Prueba ---\")\n",
    "# Imprimir los resultados de forma legible\n",
    "for res in results:\n",
    "    print(json.dumps(res, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5357b72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listado de modelos disponibles:\n",
      "- Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "- Model(name='models/gemini-1.5-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
      "                   'million tokens.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-1.5-pro-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Pro 002',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-1.5-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-1.5-flash-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
      "                   'across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
      "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-1.5-flash-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Flash 002',\n",
      "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-1.5-flash-8b',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-1.5-flash-8b-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B 001',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-1.5-flash-8b-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
      "                   'released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-2.5-pro-preview-03-25',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-03-25',\n",
      "      display_name='Gemini 2.5 Pro Preview 03-25',\n",
      "      description='Gemini 2.5 Pro Preview 03-25',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.5-flash-preview-05-20',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.5 Flash',\n",
      "      description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
      "                   'supports up to 1 million tokens, released in June of 2025.'),\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.5-flash-lite-preview-06-17',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-06-17',\n",
      "      display_name='Gemini 2.5 Flash-Lite Preview 06-17',\n",
      "      description='Preview release (June 11th, 2025) of Gemini 2.5 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.5-pro-preview-05-06',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-06',\n",
      "      display_name='Gemini 2.5 Pro Preview 05-06',\n",
      "      description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.5-pro-preview-06-05',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-06-05',\n",
      "      display_name='Gemini 2.5 Pro Preview',\n",
      "      description='Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.5-pro',\n",
      "      base_model_id='',\n",
      "      version='2.5',\n",
      "      display_name='Gemini 2.5 Pro',\n",
      "      description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.0-flash-exp',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash Experimental',\n",
      "      description='Gemini 2.0 Flash Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-2.0-flash',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash',\n",
      "      description='Gemini 2.0 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-2.0-flash-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash 001',\n",
      "      description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in January of 2025.'),\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-2.0-flash-lite-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash-Lite 001',\n",
      "      description='Stable version of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-2.0-flash-lite',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash-Lite',\n",
      "      description='Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
      "      base_model_id='',\n",
      "      version='preview-02-05',\n",
      "      display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
      "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-2.0-flash-lite-preview',\n",
      "      base_model_id='',\n",
      "      version='preview-02-05',\n",
      "      display_name='Gemini 2.0 Flash-Lite Preview',\n",
      "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "- Model(name='models/gemini-2.0-pro-exp',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini 2.0 Pro Experimental',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.0-pro-exp-02-05',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini 2.0 Pro Experimental 02-05',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-exp-1206',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.0-flash-thinking-exp',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.5-flash-preview-tts',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Preview TTS',\n",
      "      description='Gemini 2.5 Flash Preview TTS',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=16384,\n",
      "      supported_generation_methods=['countTokens', 'generateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.5-pro-preview-tts',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
      "      display_name='Gemini 2.5 Pro Preview TTS',\n",
      "      description='Gemini 2.5 Pro Preview TTS',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=16384,\n",
      "      supported_generation_methods=['countTokens', 'generateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/learnlm-2.0-flash-experimental',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='LearnLM 2.0 Flash Experimental',\n",
      "      description='LearnLM 2.0 Flash Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=32768,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemma-3-1b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 1B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemma-3-4b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 4B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemma-3-12b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 12B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemma-3-27b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 27B',\n",
      "      description='',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemma-3n-e4b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3n E4B',\n",
      "      description='',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemma-3n-e2b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3n E2B',\n",
      "      description='',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.5-flash-lite',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.5 Flash-Lite',\n",
      "      description='Stable verion of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "- Model(name='models/text-embedding-004',\n",
      "      base_model_id='',\n",
      "      version='004',\n",
      "      display_name='Text Embedding 004',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "- Model(name='models/gemini-embedding-exp-03-07',\n",
      "      base_model_id='',\n",
      "      version='exp-03-07',\n",
      "      display_name='Gemini Embedding Experimental 03-07',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "- Model(name='models/gemini-embedding-exp',\n",
      "      base_model_id='',\n",
      "      version='exp-03-07',\n",
      "      display_name='Gemini Embedding Experimental',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "- Model(name='models/gemini-embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "- Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n",
      "- Model(name='models/imagen-3.0-generate-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Imagen 3.0 002 model',\n",
      "      description='Vertex served Imagen 3.0 002 model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "- Model(name='models/imagen-4.0-generate-preview-06-06',\n",
      "      base_model_id='',\n",
      "      version='01',\n",
      "      display_name='Imagen 4 (Preview)',\n",
      "      description='Vertex served Imagen 4.0 model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "- Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
      "      base_model_id='',\n",
      "      version='01',\n",
      "      display_name='Imagen 4 Ultra (Preview)',\n",
      "      description='Vertex served Imagen 4.0 ultra model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "- Model(name='models/veo-2.0-generate-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Veo 2',\n",
      "      description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
      "                   'enabled on the associated Google Cloud Platform account. Please visit '\n",
      "                   'https://console.cloud.google.com/billing to enable it.'),\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "- Model(name='models/veo-3.0-generate-preview',\n",
      "      base_model_id='',\n",
      "      version='3.0',\n",
      "      display_name='Veo 3',\n",
      "      description='Veo 3 preview.',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "- Model(name='models/veo-3.0-fast-generate-preview',\n",
      "      base_model_id='',\n",
      "      version='3.0',\n",
      "      display_name='Veo 3 fast',\n",
      "      description='Veo 3 fast preview.',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "- Model(name='models/gemini-2.5-flash-preview-native-audio-dialog',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
      "      description='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.5-flash-exp-native-audio-thinking-dialog',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-exp-native-audio-thinking-dialog-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
      "      description='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.0-flash-live-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.0 Flash 001',\n",
      "      description='Gemini 2.0 Flash 001',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-live-2.5-flash-preview',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Live 2.5 Flash Preview',\n",
      "      description='Gemini Live 2.5 Flash Preview',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "- Model(name='models/gemini-2.5-flash-live-preview',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.5 Flash Live Preview',\n",
      "      description='Gemini 2.5 Flash Live Preview',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n"
     ]
    }
   ],
   "source": [
    "genai.list_models\n",
    "print(\"Listado de modelos disponibles:\")\n",
    "models = genai.list_models()\n",
    "for model in models:\n",
    "    print(f\"- {model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
