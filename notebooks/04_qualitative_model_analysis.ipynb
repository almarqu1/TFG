{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce5c7074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuración cargada.\n",
      "   - Modelo Base: Qwen/Qwen3-4B-Instruct-2507\n",
      "   - Adaptadores LoRA (v4): ..\\models/distilmatch_v4_silver_balanced_reasoning\\final_checkpoint\n",
      "   - Datos de Test (JSONL): ..\\data/03_gold_standard/gold_standard_test.jsonl\n",
      "   - Resultados de Evaluación (CSV): ../outputs/reports/evaluation_details_v4_silver_balanced_reasoning.csv\n"
     ]
    }
   ],
   "source": [
    "# Celda 1: Importaciones y Configuración Inicial (Versión Final Sincronizada)\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import re # Añadimos 're' aquí para que esté disponible en todo el notebook\n",
    "\n",
    "# Cargar la configuración del proyecto\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# --- Rutas y Parámetros del Proyecto ---\n",
    "MODEL_BASE_ID = config['student_model']['base_model_name']\n",
    "experiment_v4_config = config['experiments']['v4_silver_balanced_reasoning']\n",
    "\n",
    "# Ruta al modelo v4 (corregida con 'final_checkpoint')\n",
    "lora_base_path = os.path.join('..', experiment_v4_config['output_dir'])\n",
    "LORA_ADAPTER_PATH_V4 = os.path.join(lora_base_path, 'final_checkpoint')\n",
    "\n",
    "# Ruta a los datos de test (JSONL con los prompts completos)\n",
    "TEST_DATA_PATH_JSONL = os.path.join('..', config['data_paths']['gold_standard']['test_jsonl'])\n",
    "\n",
    "# --- ¡ACTUALIZADO! Ruta al CSV con los resultados de la evaluación ---\n",
    "EVAL_RESULTS_PATH_CSV = '../outputs/reports/evaluation_details_v4_silver_balanced_reasoning.csv' \n",
    "\n",
    "print(\"✅ Configuración cargada.\")\n",
    "print(f\"   - Modelo Base: {MODEL_BASE_ID}\")\n",
    "print(f\"   - Adaptadores LoRA (v4): {LORA_ADAPTER_PATH_V4}\")\n",
    "print(f\"   - Datos de Test (JSONL): {TEST_DATA_PATH_JSONL}\")\n",
    "print(f\"   - Resultados de Evaluación (CSV): {EVAL_RESULTS_PATH_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c6fd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Cargando modelo y tokenizer... (Puede tardar unos minutos)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367b1d2004be4844811650a25d34677b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ¡Modelo v4 listo para la inferencia!\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: Cargar el Modelo y el Tokenizer\n",
    "\n",
    "print(\"⏳ Cargando modelo y tokenizer... (Puede tardar unos minutos)\")\n",
    "\n",
    "# Configuración de cuantización en 4-bit (consistente con el entrenamiento)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Cargar el modelo base desde Hugging Face Hub\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_BASE_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" # Automáticamente usa la GPU si está disponible\n",
    ")\n",
    "\n",
    "# Cargar el tokenizer asociado al modelo base\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_BASE_ID)\n",
    "# Es importante asegurarse de que el pad_token es el eos_token para modelos decoder-only\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Cargar y fusionar los adaptadores LoRA del modelo v4\n",
    "# PeftModel es la librería que se encarga de aplicar los adaptadores al modelo base.\n",
    "model_v4 = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH_V4)\n",
    "\n",
    "print(\"✅ ¡Modelo v4 listo para la inferencia!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bca2ba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cargados 44 ejemplos del conjunto de test.\n",
      "Vista previa de los datos:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an automated scoring system. Your SOLE...</td>\n",
       "      <td>Score: 45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an automated scoring system. Your SOLE...</td>\n",
       "      <td>Score: 70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an automated scoring system. Your SOLE...</td>\n",
       "      <td>Score: 70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an automated scoring system. Your SOLE...</td>\n",
       "      <td>Score: 15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an automated scoring system. Your SOLE...</td>\n",
       "      <td>Score: 45.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt     response\n",
       "0  You are an automated scoring system. Your SOLE...  Score: 45.0\n",
       "1  You are an automated scoring system. Your SOLE...  Score: 70.0\n",
       "2  You are an automated scoring system. Your SOLE...  Score: 70.0\n",
       "3  You are an automated scoring system. Your SOLE...  Score: 15.0\n",
       "4  You are an automated scoring system. Your SOLE...  Score: 45.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Celda 3: Cargar los Datos de Test\n",
    "\n",
    "# Cargar el archivo JSONL en un DataFrame de pandas para facilitar la manipulación\n",
    "test_df = pd.read_json(TEST_DATA_PATH, lines=True)\n",
    "\n",
    "print(f\"✅ Cargados {len(test_df)} ejemplos del conjunto de test.\")\n",
    "print(\"Vista previa de los datos:\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b170bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4: La Función de Inferencia \"Thinking Mode\" (Fiel a la Evaluación)\n",
    "\n",
    "def get_v4_prediction(cv_text: str, job_description_text: str, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Toma un CV y una oferta, y genera el razonamiento JSON usando el formato exacto\n",
    "    del pipeline de evaluación.\n",
    "    \"\"\"\n",
    "    # 1. System Prompt: Exactamente como en tu script de evaluación.\n",
    "    system_prompt = \"\"\"As an expert HR analyst, provide a step-by-step analysis of the compatibility between the following CV and Job Offer. \n",
    "Your response must be a JSON object with the following structure:\n",
    "{\n",
    "    \"strengths\": \"A brief analysis of the candidate's strengths.\",\n",
    "    \"concerns_and_gaps\": \"A brief analysis of the candidate's weaknesses or gaps.\",\n",
    "    \"verdict\": \"One of the following categories: 'MUST INTERVIEW', 'PROMISING FIT', 'BORDERLINE', 'NO FIT'.\",\n",
    "    \"score\": A numerical score from 0 to 100.\n",
    "}\n",
    "The JSON object should be enclosed in ```json ... ```.\"\"\"\n",
    "\n",
    "    # 2. User Prompt: Usa los placeholders {cv} y {job_description}.\n",
    "    user_prompt_template = \"\"\"[CV]\n",
    "{cv}\n",
    "\n",
    "[OFERTA DE TRABAJO]\n",
    "{job_description}\"\"\"\n",
    "\n",
    "    # 3. Rellenamos la plantilla del usuario.\n",
    "    final_user_prompt = user_prompt_template.format(cv=cv_text, job_description=job_description_text)\n",
    "    \n",
    "    # 4. Construimos el formato de chat que espera el modelo.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": final_user_prompt}\n",
    "    ]\n",
    "    \n",
    "    # 5. Preparamos y ejecutamos la inferencia.\n",
    "    full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\", return_attention_mask=True).to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=400, pad_token_id=tokenizer.eos_token_id)\n",
    "    raw_output = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # 6. Extraemos y parseamos el JSON de la respuesta.\n",
    "    try:\n",
    "        # Buscamos el bloque de código JSON ```json ... ```\n",
    "        json_match = re.search(r\"```json\\s*\\n(.*?)\\n\\s*```\", raw_output, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(1)\n",
    "            parsed_json = json.loads(json_str)\n",
    "            return parsed_json, raw_output\n",
    "        else:\n",
    "            # Si no encuentra el bloque, intenta parsear la salida cruda\n",
    "            parsed_json = json.loads(raw_output)\n",
    "            return parsed_json, raw_output\n",
    "            \n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        return {\"error\": \"Failed to parse JSON from model output\"}, raw_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c79f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🔬 ANALIZANDO CASO DE ESTUDIO (Índice: 0)\n",
      "============================================================\n",
      "🎯 Score Real (Humano):          45.0\n",
      "📊 Score Predicho (del CSV):     85.0\n",
      "------------------------------------------------------------\n",
      "⏳ Generando razonamiento en vivo del modelo v4 para este caso...\n",
      "\n",
      "============================================================\n",
      "🤖 RAZONAMIENTO EN VIVO DEL MODELO v4\n",
      "============================================================\n",
      "📊 Score (en vivo):              55\n",
      "⚖️ Veredicto (en vivo):          BORDERLINE\n",
      "\n",
      "✅ Fortalezas Identificadas:\n",
      "   The candidate has over 4 years of experience in data analysis and business intelligence, which aligns well with the core responsibilities of a Data Analyst. Their proficiency in Python, Excel, and data visualization tools suggests a strong analytical foundation. Additionally, their background in machine learning and AI indicates a technical aptitude that could be beneficial in a data-driven role.\n",
      "\n",
      "🤔 Preocupaciones y Brechas Identificadas:\n",
      "   The candidate's experience is significantly mismatched with the job title and level. The role is explicitly an Entry-Level Business Analyst/Product Owner requiring experience in requirement gathering, Agile methodologies, and SDLC, which are absent from their CV. There is no mention of business analysis, user stories, roadmaps, or communication with leadership, making their profile technically strong but functionally misaligned for this role.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Celda 5: El Laboratorio de Análisis (Versión Final y Sincronizada)\n",
    "\n",
    "# --- 1. CARGAMOS LOS DOS ARCHIVOS DE DATOS ---\n",
    "# El CSV contiene los scores que ya calculaste.\n",
    "eval_df = pd.read_csv(EVAL_RESULTS_PATH_CSV)\n",
    "\n",
    "# El JSONL contiene los prompts originales con los textos completos.\n",
    "# Es importante que las filas de ambos archivos estén en el mismo orden.\n",
    "prompts_df = pd.read_json(TEST_DATA_PATH_JSONL, lines=True)\n",
    "\n",
    "\n",
    "# --- 2. SELECCIONA EL CASO A ESTUDIAR ---\n",
    "index_to_analyze = 0 # <--- ¡CAMBIA ESTE NÚMERO PARA EXPLORAR!\n",
    "\n",
    "\n",
    "# 3. EXTRAE LA INFORMACIÓN DE CADA FUENTE USANDO EL MISMO ÍNDICE\n",
    "try:\n",
    "    # De nuestro CSV, cogemos los scores.\n",
    "    sample_from_csv = eval_df.iloc[index_to_analyze]\n",
    "    true_score = sample_from_csv['true_score']\n",
    "    predicted_score_from_csv = sample_from_csv['predicted_score']\n",
    "\n",
    "    # De nuestro JSONL, cogemos el prompt para extraer los textos.\n",
    "    sample_from_jsonl = prompts_df.iloc[index_to_analyze]\n",
    "    prompt_full_text = sample_from_jsonl['prompt']\n",
    "\n",
    "    # Extraemos los textos del CV y la Oferta usando el método split() que ya validamos.\n",
    "    text_after_cv_tag = prompt_full_text.split('[CV]')[1]\n",
    "    parts = text_after_cv_tag.split('[OFERTA DE TRABAJO]')\n",
    "    cv_text = parts[0].strip()\n",
    "    # ¡Importante! Usamos 'job_description_text' para que coincida con la función de la Celda 4.\n",
    "    job_description_text = parts[1].strip()\n",
    "\n",
    "    # --- 4. MOSTRAMOS EL ANÁLISIS ---\n",
    "    print(\"=\"*60)\n",
    "    print(f\"🔬 ANALIZANDO CASO DE ESTUDIO (Índice: {index_to_analyze})\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"🎯 Score Real (Humano):          {true_score}\")\n",
    "    print(f\"📊 Score Predicho (del CSV):     {predicted_score_from_csv}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"⏳ Generando razonamiento en vivo del modelo v4 para este caso...\")\n",
    "\n",
    "    # 5. Generamos el razonamiento en vivo llamando a la función de la Celda 4\n",
    "    predicted_json_live, raw_output_live = get_v4_prediction(\n",
    "        cv_text=cv_text, \n",
    "        job_description_text=job_description_text, \n",
    "        model=model_v4, \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # 6. Mostramos el razonamiento detallado\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🤖 RAZONAMIENTO EN VIVO DEL MODELO v4\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if \"error\" in predicted_json_live:\n",
    "        print(\"❌ ¡ERROR! El modelo no generó un JSON válido en esta ejecución.\")\n",
    "        print(\"\\n--- Salida Cruda del Modelo ---\")\n",
    "        print(raw_output_live)\n",
    "    else:\n",
    "        print(f\"📊 Score (en vivo):              {predicted_json_live.get('score', 'N/A')}\")\n",
    "        print(f\"⚖️ Veredicto (en vivo):          {predicted_json_live.get('verdict', 'N/A')}\")\n",
    "        print(\"\\n✅ Fortalezas Identificadas:\")\n",
    "        print(f\"   {predicted_json_live.get('strengths', 'N/A')}\")\n",
    "        print(\"\\n🤔 Preocupaciones y Brechas Identificadas:\")\n",
    "        print(f\"   {predicted_json_live.get('concerns_and_gaps', 'N/A')}\")\n",
    "\n",
    "except IndexError:\n",
    "    print(f\"❌ Error: El índice {index_to_analyze} está fuera de rango. Elige un número entre 0 y {len(eval_df)-1}.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Ocurrió un error inesperado durante el análisis: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
