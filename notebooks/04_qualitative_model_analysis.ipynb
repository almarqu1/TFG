{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce5c7074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuraci√≥n cargada.\n",
      "   - Modelo Base: Qwen/Qwen3-4B-Instruct-2507\n",
      "   - Adaptadores LoRA (v4): ..\\models/distilmatch_v4_silver_balanced_reasoning\\final_checkpoint\n",
      "   - Datos de Test (JSONL): ..\\data/03_gold_standard/gold_standard_test.jsonl\n",
      "   - Resultados de Evaluaci√≥n (CSV): ../outputs/reports/evaluation_details_v4_silver_balanced_reasoning.csv\n"
     ]
    }
   ],
   "source": [
    "# Celda 1: Importaciones y Configuraci√≥n Inicial (Versi√≥n Final Sincronizada)\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import re # A√±adimos 're' aqu√≠ para que est√© disponible en todo el notebook\n",
    "\n",
    "# Cargar la configuraci√≥n del proyecto\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# --- Rutas y Par√°metros del Proyecto ---\n",
    "MODEL_BASE_ID = config['student_model']['base_model_name']\n",
    "experiment_v4_config = config['experiments']['v4_silver_balanced_reasoning']\n",
    "\n",
    "# Ruta al modelo v4 (corregida con 'final_checkpoint')\n",
    "lora_base_path = os.path.join('..', experiment_v4_config['output_dir'])\n",
    "LORA_ADAPTER_PATH_V4 = os.path.join(lora_base_path, 'final_checkpoint')\n",
    "\n",
    "# Ruta a los datos de test (JSONL con los prompts completos)\n",
    "TEST_DATA_PATH_JSONL = os.path.join('..', config['data_paths']['gold_standard']['test_jsonl'])\n",
    "\n",
    "# --- ¬°ACTUALIZADO! Ruta al CSV con los resultados de la evaluaci√≥n ---\n",
    "EVAL_RESULTS_PATH_CSV = '../outputs/reports/evaluation_details_v4_silver_balanced_reasoning.csv' \n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n cargada.\")\n",
    "print(f\"   - Modelo Base: {MODEL_BASE_ID}\")\n",
    "print(f\"   - Adaptadores LoRA (v4): {LORA_ADAPTER_PATH_V4}\")\n",
    "print(f\"   - Datos de Test (JSONL): {TEST_DATA_PATH_JSONL}\")\n",
    "print(f\"   - Resultados de Evaluaci√≥n (CSV): {EVAL_RESULTS_PATH_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c6fd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Cargando modelo y tokenizer... (Puede tardar unos minutos)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367b1d2004be4844811650a25d34677b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ¬°Modelo v4 listo para la inferencia!\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: Cargar el Modelo y el Tokenizer\n",
    "\n",
    "print(\"‚è≥ Cargando modelo y tokenizer... (Puede tardar unos minutos)\")\n",
    "\n",
    "# Configuraci√≥n de cuantizaci√≥n en 4-bit (consistente con el entrenamiento)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Cargar el modelo base desde Hugging Face Hub\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_BASE_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" # Autom√°ticamente usa la GPU si est√° disponible\n",
    ")\n",
    "\n",
    "# Cargar el tokenizer asociado al modelo base\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_BASE_ID)\n",
    "# Es importante asegurarse de que el pad_token es el eos_token para modelos decoder-only\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Cargar y fusionar los adaptadores LoRA del modelo v4\n",
    "# PeftModel es la librer√≠a que se encarga de aplicar los adaptadores al modelo base.\n",
    "model_v4 = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH_V4)\n",
    "\n",
    "print(\"‚úÖ ¬°Modelo v4 listo para la inferencia!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bca2ba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cargados 44 ejemplos del conjunto de test.\n",
      "Vista previa de los datos:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an automated scoring system. Your SOLE...</td>\n",
       "      <td>Score: 45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an automated scoring system. Your SOLE...</td>\n",
       "      <td>Score: 70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an automated scoring system. Your SOLE...</td>\n",
       "      <td>Score: 70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an automated scoring system. Your SOLE...</td>\n",
       "      <td>Score: 15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an automated scoring system. Your SOLE...</td>\n",
       "      <td>Score: 45.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt     response\n",
       "0  You are an automated scoring system. Your SOLE...  Score: 45.0\n",
       "1  You are an automated scoring system. Your SOLE...  Score: 70.0\n",
       "2  You are an automated scoring system. Your SOLE...  Score: 70.0\n",
       "3  You are an automated scoring system. Your SOLE...  Score: 15.0\n",
       "4  You are an automated scoring system. Your SOLE...  Score: 45.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Celda 3: Cargar los Datos de Test\n",
    "\n",
    "# Cargar el archivo JSONL en un DataFrame de pandas para facilitar la manipulaci√≥n\n",
    "test_df = pd.read_json(TEST_DATA_PATH, lines=True)\n",
    "\n",
    "print(f\"‚úÖ Cargados {len(test_df)} ejemplos del conjunto de test.\")\n",
    "print(\"Vista previa de los datos:\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b170bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4: La Funci√≥n de Inferencia \"Thinking Mode\" (Fiel a la Evaluaci√≥n)\n",
    "\n",
    "def get_v4_prediction(cv_text: str, job_description_text: str, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Toma un CV y una oferta, y genera el razonamiento JSON usando el formato exacto\n",
    "    del pipeline de evaluaci√≥n.\n",
    "    \"\"\"\n",
    "    # 1. System Prompt: Exactamente como en tu script de evaluaci√≥n.\n",
    "    system_prompt = \"\"\"As an expert HR analyst, provide a step-by-step analysis of the compatibility between the following CV and Job Offer. \n",
    "Your response must be a JSON object with the following structure:\n",
    "{\n",
    "    \"strengths\": \"A brief analysis of the candidate's strengths.\",\n",
    "    \"concerns_and_gaps\": \"A brief analysis of the candidate's weaknesses or gaps.\",\n",
    "    \"verdict\": \"One of the following categories: 'MUST INTERVIEW', 'PROMISING FIT', 'BORDERLINE', 'NO FIT'.\",\n",
    "    \"score\": A numerical score from 0 to 100.\n",
    "}\n",
    "The JSON object should be enclosed in ```json ... ```.\"\"\"\n",
    "\n",
    "    # 2. User Prompt: Usa los placeholders {cv} y {job_description}.\n",
    "    user_prompt_template = \"\"\"[CV]\n",
    "{cv}\n",
    "\n",
    "[OFERTA DE TRABAJO]\n",
    "{job_description}\"\"\"\n",
    "\n",
    "    # 3. Rellenamos la plantilla del usuario.\n",
    "    final_user_prompt = user_prompt_template.format(cv=cv_text, job_description=job_description_text)\n",
    "    \n",
    "    # 4. Construimos el formato de chat que espera el modelo.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": final_user_prompt}\n",
    "    ]\n",
    "    \n",
    "    # 5. Preparamos y ejecutamos la inferencia.\n",
    "    full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\", return_attention_mask=True).to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=400, pad_token_id=tokenizer.eos_token_id)\n",
    "    raw_output = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # 6. Extraemos y parseamos el JSON de la respuesta.\n",
    "    try:\n",
    "        # Buscamos el bloque de c√≥digo JSON ```json ... ```\n",
    "        json_match = re.search(r\"```json\\s*\\n(.*?)\\n\\s*```\", raw_output, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(1)\n",
    "            parsed_json = json.loads(json_str)\n",
    "            return parsed_json, raw_output\n",
    "        else:\n",
    "            # Si no encuentra el bloque, intenta parsear la salida cruda\n",
    "            parsed_json = json.loads(raw_output)\n",
    "            return parsed_json, raw_output\n",
    "            \n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        return {\"error\": \"Failed to parse JSON from model output\"}, raw_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c79f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üî¨ ANALIZANDO CASO DE ESTUDIO (√çndice: 0)\n",
      "============================================================\n",
      "üéØ Score Real (Humano):          45.0\n",
      "üìä Score Predicho (del CSV):     85.0\n",
      "------------------------------------------------------------\n",
      "‚è≥ Generando razonamiento en vivo del modelo v4 para este caso...\n",
      "\n",
      "============================================================\n",
      "ü§ñ RAZONAMIENTO EN VIVO DEL MODELO v4\n",
      "============================================================\n",
      "üìä Score (en vivo):              55\n",
      "‚öñÔ∏è Veredicto (en vivo):          BORDERLINE\n",
      "\n",
      "‚úÖ Fortalezas Identificadas:\n",
      "   The candidate has over 4 years of experience in data analysis and business intelligence, which aligns well with the core responsibilities of a Data Analyst. Their proficiency in Python, Excel, and data visualization tools suggests a strong analytical foundation. Additionally, their background in machine learning and AI indicates a technical aptitude that could be beneficial in a data-driven role.\n",
      "\n",
      "ü§î Preocupaciones y Brechas Identificadas:\n",
      "   The candidate's experience is significantly mismatched with the job title and level. The role is explicitly an Entry-Level Business Analyst/Product Owner requiring experience in requirement gathering, Agile methodologies, and SDLC, which are absent from their CV. There is no mention of business analysis, user stories, roadmaps, or communication with leadership, making their profile technically strong but functionally misaligned for this role.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Celda 5: El Laboratorio de An√°lisis (Versi√≥n Final y Sincronizada)\n",
    "\n",
    "# --- 1. CARGAMOS LOS DOS ARCHIVOS DE DATOS ---\n",
    "# El CSV contiene los scores que ya calculaste.\n",
    "eval_df = pd.read_csv(EVAL_RESULTS_PATH_CSV)\n",
    "\n",
    "# El JSONL contiene los prompts originales con los textos completos.\n",
    "# Es importante que las filas de ambos archivos est√©n en el mismo orden.\n",
    "prompts_df = pd.read_json(TEST_DATA_PATH_JSONL, lines=True)\n",
    "\n",
    "\n",
    "# --- 2. SELECCIONA EL CASO A ESTUDIAR ---\n",
    "index_to_analyze = 0 # <--- ¬°CAMBIA ESTE N√öMERO PARA EXPLORAR!\n",
    "\n",
    "\n",
    "# 3. EXTRAE LA INFORMACI√ìN DE CADA FUENTE USANDO EL MISMO √çNDICE\n",
    "try:\n",
    "    # De nuestro CSV, cogemos los scores.\n",
    "    sample_from_csv = eval_df.iloc[index_to_analyze]\n",
    "    true_score = sample_from_csv['true_score']\n",
    "    predicted_score_from_csv = sample_from_csv['predicted_score']\n",
    "\n",
    "    # De nuestro JSONL, cogemos el prompt para extraer los textos.\n",
    "    sample_from_jsonl = prompts_df.iloc[index_to_analyze]\n",
    "    prompt_full_text = sample_from_jsonl['prompt']\n",
    "\n",
    "    # Extraemos los textos del CV y la Oferta usando el m√©todo split() que ya validamos.\n",
    "    text_after_cv_tag = prompt_full_text.split('[CV]')[1]\n",
    "    parts = text_after_cv_tag.split('[OFERTA DE TRABAJO]')\n",
    "    cv_text = parts[0].strip()\n",
    "    # ¬°Importante! Usamos 'job_description_text' para que coincida con la funci√≥n de la Celda 4.\n",
    "    job_description_text = parts[1].strip()\n",
    "\n",
    "    # --- 4. MOSTRAMOS EL AN√ÅLISIS ---\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üî¨ ANALIZANDO CASO DE ESTUDIO (√çndice: {index_to_analyze})\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üéØ Score Real (Humano):          {true_score}\")\n",
    "    print(f\"üìä Score Predicho (del CSV):     {predicted_score_from_csv}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"‚è≥ Generando razonamiento en vivo del modelo v4 para este caso...\")\n",
    "\n",
    "    # 5. Generamos el razonamiento en vivo llamando a la funci√≥n de la Celda 4\n",
    "    predicted_json_live, raw_output_live = get_v4_prediction(\n",
    "        cv_text=cv_text, \n",
    "        job_description_text=job_description_text, \n",
    "        model=model_v4, \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # 6. Mostramos el razonamiento detallado\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ü§ñ RAZONAMIENTO EN VIVO DEL MODELO v4\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if \"error\" in predicted_json_live:\n",
    "        print(\"‚ùå ¬°ERROR! El modelo no gener√≥ un JSON v√°lido en esta ejecuci√≥n.\")\n",
    "        print(\"\\n--- Salida Cruda del Modelo ---\")\n",
    "        print(raw_output_live)\n",
    "    else:\n",
    "        print(f\"üìä Score (en vivo):              {predicted_json_live.get('score', 'N/A')}\")\n",
    "        print(f\"‚öñÔ∏è Veredicto (en vivo):          {predicted_json_live.get('verdict', 'N/A')}\")\n",
    "        print(\"\\n‚úÖ Fortalezas Identificadas:\")\n",
    "        print(f\"   {predicted_json_live.get('strengths', 'N/A')}\")\n",
    "        print(\"\\nü§î Preocupaciones y Brechas Identificadas:\")\n",
    "        print(f\"   {predicted_json_live.get('concerns_and_gaps', 'N/A')}\")\n",
    "\n",
    "except IndexError:\n",
    "    print(f\"‚ùå Error: El √≠ndice {index_to_analyze} est√° fuera de rango. Elige un n√∫mero entre 0 y {len(eval_df)-1}.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ocurri√≥ un error inesperado durante el an√°lisis: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
